/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/lightning_fabric/__init__.py:40: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
wandb: Currently logged in as: chriszhang09 (chriszhang09-new-york-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /scratch/cz2200/aib9/wandb/run-20251011_170516-kwwhzim1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-deluge-166
wandb: ⭐️ View project at https://wandb.ai/chriszhang09-new-york-university/aib9-vae-pairwise
wandb: 🚀 View run at https://wandb.ai/chriszhang09-new-york-university/aib9-vae-pairwise/runs/kwwhzim1
/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:1032: UserWarning: 'NeighborEmbedding.jittable' is deprecated and a no-op. Please remove its usage.
  warnings.warn(f"'{self.__class__.__name__}.jittable' is deprecated "
/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:1032: UserWarning: 'EdgeEmbedding.jittable' is deprecated and a no-op. Please remove its usage.
  warnings.warn(f"'{self.__class__.__name__}.jittable' is deprecated "
/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:1032: UserWarning: 'ViS_MP_Vertex_Edge.jittable' is deprecated and a no-op. Please remove its usage.
  warnings.warn(f"'{self.__class__.__name__}.jittable' is deprecated "
Using CUDA device: Quadro RTX 8000
CUDA memory: 47.8 GB
Successfully initialized bias for log_var outputs to: -3.00

============================================================
Starting Training - 1000000 samples
Model parameters: 16,594,051
Loss type: MSE (coordinate-based)
============================================================

Traceback (most recent call last):
  File "/scratch/cz2200/aib9/train_vae_mse_large.py", line 415, in <module>
    main()
  File "/scratch/cz2200/aib9/train_vae_mse_large.py", line 275, in main
    recon_batch, mu, log_var = model(molecules)
  File "/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/scratch/cz2200/aib9/mse_training/vae_model_mse.py", line 74, in forward
    reconstructed_pos = self.decoder(z, atom_types_one_hot, None, data.batch)
  File "/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/cz2200/aib9/mse_training/vae_decoder_mse.py", line 251, in forward
    node_features, coords = layer(node_features, coords, edge_index)
  File "/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ext3/miniforge3/envs/visnet/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/cz2200/aib9/mse_training/vae_decoder_mse.py", line 88, in forward
    messages = self.message(x, pos, row, col)
  File "/scratch/cz2200/aib9/mse_training/vae_decoder_mse.py", line 121, in message
    edge_features = torch.cat([x_i, x_j, dist_sq], dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 416.00 MiB. GPU 0 has a total capacty of 44.48 GiB of which 391.31 MiB is free. Including non-PyTorch memory, this process has 44.10 GiB memory in use. Of the allocated memory 43.28 GiB is allocated by PyTorch, and 636.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mpeachy-deluge-166[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251011_170516-kwwhzim1/logs[0m
